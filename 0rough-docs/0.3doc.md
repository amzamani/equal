3. **Architecture Document**

   Name this file `ARCHITECTURE.md` and place it in the root of your repo.

   Write a short document (1-2 pages) with diagrams illustrating your data model and system design. Include a brief API spec—endpoint definitions, request/response shapes. Make it easy to skim.

   Keep it **as short and concise as possible**. We value clear, direct technical writing. AI tools are fine for formatting and polish, but the substance and reasoning must be yours. We're allergic to generic, hand-wavy architecture docs. We will reject candidates with AI slop that hasn't been verified by a human. The intent is to test your ability to think and reason about the problem.

   We don't expect you to implement and take care of all of these things in the actual SDK code or the API. But we'd love your thoughts on how you deal with these things, in terms of questions which we pointed out from an architecture perspective.

   Address the following:

   **Core Design (required):**

   - **Data Model Rationale:** Your architecture doc shows your data model. Explain *why* you structured it this way. What alternatives did you consider? What would break if you'd made different choices?

   - **Debugging Walkthrough:** A competitor selection run returns a bad match—a **phone case** matched against a **laptop stand**. Using your X-Ray system, how would someone figure out where things went wrong? Be specific about what they'd see and query.

   **Queryability**

   - Your system will be used across multiple different pipelines (competitor selection, listing optimization, categorization, etc.), each with different steps. A user wants to ask: "Show me all runs where the filtering step eliminated more than 90% of candidates"—regardless of which pipeline it was. How does your data model and query API support this? What constraints or conventions do you impose on developers to make this possible? Also think about variability in the context of those as well. These are use cases which we have given, but an X-ray system like this could be deployable at a million other use cases! Think about queryability in the context of those as well.

   **Performance & Scale**

   - Consider a step that takes 5,000 candidates as input and filters down to 30. Capturing full details for all 5,000 (including rejection reasons) might be prohibitively expensive. How does your system handle this? Describe the trade-offs between completeness, performance, and storage cost. Who decides what gets captured in full vs. summarized—the system or the developer?

   **Developer Experience**

   - Imagine a developer has an existing pipeline they want to instrument. Walk us through what changes they need to make to their code. Specifically: (a) What's the minimal instrumentation to get *something* useful? (b) What does full instrumentation look like? (c) What happens to the pipeline if the X-Ray backend is unavailable?

   **Real-World Application:**

   - you've worked on Inngest where X-Ray-style visibility would have saved debugging time. How would you retrofit this Xray solution into that system? 

   **What Next??**

   - If you were to ship this SDK for real world use cases, what are other technical aspects you would want to work on?